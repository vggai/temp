To perform the steps of using Unsupervised Deep Embedded Clustering (DEC) to group skills from user documents in reality, let's outline a practical approach:

### 1. Data Collection and Preprocessing
- **Collect Documents**: Gather all the user documents you want to analyze. These could be resumes, job descriptions, portfolios, etc.
- **Clean Data**: Use text preprocessing techniques to clean your documents. This includes removing special characters, stop words, and possibly stemming (reducing words to their base form).
- **Tokenization**: Break down the text into individual elements or tokens (e.g., words).

### 2. Feature Extraction
- **Choose a Method**: Decide on a method for converting text to numerical features. Options include TF-IDF, word embeddings (like Word2Vec or GloVe), or using a pre-trained language model (e.g., BERT) to extract contextual embeddings.
- **Apply the Method**: Convert your cleaned and tokenized text into a numerical format using the chosen method, resulting in a feature matrix where each document is represented by a vector of numbers.

### 3. Pre-training the Model
- **Select a Model**: Choose a deep learning architecture suitable for your data and task. Autoencoders are commonly used for pre-training in DEC because they're good at learning compressed, informative representations of data.
- **Train the Model**: Pre-train your model on the feature matrix to learn a lower-dimensional representation of your documents. This step is crucial for capturing the underlying structure of your data.

### 4. Cluster Initialization
- **Initial Clustering**: Use a conventional clustering algorithm like k-means on the learned representations to form initial clusters. This step provides a starting point for the embedded clustering process.

### 5. Deep Embedded Clustering
- **Integrate Clustering into the Model**: Modify your pre-trained model to include a clustering layer or objective, aiming to refine both the document representations and cluster assignments simultaneously.
- **Optimization**: Through iterative optimization, adjust the parameters of your model and the cluster assignments based on a clustering loss function. This loss function typically measures how well the documents fit into their assigned clusters and tries to improve this fit over time.

### 6. Finalizing Clusters
- **Convergence**: Continue the optimization process until the changes in cluster assignments diminish over iterations, indicating that the model has found a stable grouping of the documents.

### 7. Interpretation and Analysis
- **Analyze Clusters**: Examine the documents in each cluster to understand the common skills they represent. This might involve looking at the most frequent terms in each cluster or using dimensionality reduction techniques (like t-SNE or PCA) to visualize the clusters.
- **Label Clusters**: Based on your analysis, assign labels to each cluster that summarize the skills they contain.

### 8. Output
- **Present Results**: The final step is to present the clusters and their labels in a meaningful way. This could be a report, an interactive dashboard, or a database that maps documents to their corresponding skill clusters.

### In Practice
Performing these steps in reality requires a mix of data science skills, including programming (likely in Python, given its rich ecosystem for data science), familiarity with machine learning and deep learning libraries (such as TensorFlow or PyTorch), and the ability to interpret and validate the results of your clustering analysis. Additionally, you would need to iterate over these steps, tuning parameters and possibly revisiting your feature extraction and model choice, to improve the quality of your clusters.